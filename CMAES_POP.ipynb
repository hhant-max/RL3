{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMAES_POP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMYA0LXuvs5K2+cF+NmMbyl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hhant-max/RL3/blob/main/CMAES_POP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import numpy as np\n",
        "import copy\n"
      ],
      "metadata": {
        "id": "6dxFhzX-PCsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# before\n"
      ],
      "metadata": {
        "id": "TIPTIvSUSFw8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NmpiI6eO_eW"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Hyperparameters\n",
        "learning_rate = 0.0005\n",
        "gamma         = 0.98\n",
        "lmbda         = 0.95\n",
        "eps_clip      = 0.1\n",
        "K_epoch       = 3\n",
        "T_horizon     = 20\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PPO, self).__init__()\n",
        "        self.data = []\n",
        "        \n",
        "        self.fc1   = nn.Linear(4,256)\n",
        "        self.fc_pi = nn.Linear(256,2)\n",
        "        self.fc_v  = nn.Linear(256,1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def pi(self, x, softmax_dim = 0):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc_pi(x)\n",
        "        prob = F.softmax(x, dim=softmax_dim)\n",
        "        return prob\n",
        "    \n",
        "    def v(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        v = self.fc_v(x)\n",
        "        return v\n",
        "      \n",
        "    def put_data(self, transition):\n",
        "        self.data.append(transition)\n",
        "        \n",
        "    def make_batch(self):\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
        "        for transition in self.data:\n",
        "            s, a, r, s_prime, prob_a, done = transition\n",
        "            \n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            prob_a_lst.append([prob_a])\n",
        "            done_mask = 0 if done else 1\n",
        "            done_lst.append([done_mask])\n",
        "            \n",
        "        s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "                                          torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "                                          torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n",
        "        self.data = []\n",
        "        return s, a, r, s_prime, done_mask, prob_a\n",
        "        \n",
        "    def train_net(self):\n",
        "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n",
        "\n",
        "        for i in range(K_epoch):\n",
        "          # td_target is rewards\n",
        "            td_target = r + gamma * self.v(s_prime) * done_mask\n",
        "            delta = td_target - self.v(s)\n",
        "            delta = delta.detach().numpy()\n",
        "\n",
        "            advantage_lst = []\n",
        "            advantage = 0.0\n",
        "            for delta_t in delta[::-1]:\n",
        "              # GAE funciton! fitness function? min?\n",
        "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
        "                advantage_lst.append([advantage])\n",
        "            advantage_lst.reverse()\n",
        "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
        "\n",
        "            pi = self.pi(s, softmax_dim=1)\n",
        "            pi_a = pi.gather(1,a)\n",
        "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
        "\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
        "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "        \n",
        "def main():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    model = PPO()\n",
        "    score = 0.0\n",
        "    print_interval = 20\n",
        "\n",
        "    for n_epi in range(10000):\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            for t in range(T_horizon):\n",
        "                prob = model.pi(torch.from_numpy(s).float())\n",
        "                m = Categorical(prob)\n",
        "                a = m.sample().item()\n",
        "                s_prime, r, done, info = env.step(a)\n",
        "\n",
        "                model.put_data((s, a, r/100.0, s_prime, prob[a].item(), done))\n",
        "                s = s_prime\n",
        "\n",
        "                score += r\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            model.train_net()\n",
        "\n",
        "        if n_epi%print_interval==0 and n_epi!=0:\n",
        "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
        "            score = 0.0\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_env_PPO():\n",
        "  # 画图 计算 rewards？？？？\n",
        "  env = gym.make(\"CartPole-v1\")\n",
        "  MAXEPISODE = 1000\n",
        "  model = PPO()\n",
        "\n",
        "  for episode in range(MAXEPISODE):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        # for t in range(T_horizon):\n",
        "          prob = model.pi(torch.from_numpy(s).float()) #tensor([0.5188, 0.4812]\n",
        "          m = Categorical(prob)\n",
        "          a = m.sample().item()\n",
        "          # action = dist.sample().squeeze()???\n",
        "          # log_prob = Categorical(prob).log_prob(a).squeeze()\n",
        "          #\"\"\"Converts variable to numpy.\"\"\"\n",
        "          #alue, action, logprob = utils.to_data(value), utils.to_data(action), utils.to_data(logprob)\n",
        "          s_prime, r, done, info = env.step(a)\n",
        "\n",
        "          model.put_data((s, a, r/100.0, s_prime, prob[a].item(), done))\n",
        "          s = s_prime \n",
        "\n",
        "\n",
        "run_env_PPO()"
      ],
      "metadata": {
        "id": "U6rtzRu-7KKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rewards calculation\n",
        "\n",
        "rewards = []\n",
        "# network CNN return return value, log_prob, entropy\n",
        "#policy = policies.get_policy(args, env) \n",
        "policy = model.pi()\n",
        "alg= ESPPOModule(policy,run_env_PPO)\n",
        "while True:\n",
        "  weights = alg.step()\n",
        "  # caculate every 10 iterations\n",
        "  if iteration % 10 == 0 :\n",
        "    # policy returns a softmax (2d) pi\n",
        "    test_reward = run_env_PPO(policy, stochastic=False, render=False, reward_only=True)\n",
        "    rewards.append(test_reward) #states, actions, rewards, values.squeeze(), logprobs, returns"
      ],
      "metadata": {
        "id": "EJXhD30HzSdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After"
      ],
      "metadata": {
        "id": "2NzodK1ASIkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 看怎么 update\n",
        "# es ppo document\n",
        "class ESPPOModule(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      population_size = 5,\n",
        "      learning_rate = 0.95,\n",
        "\n",
        "      softmax_dim = 0,\n",
        "      sigma = 0.1,\n",
        "      n_rollout = 10, # running depth in one batch\n",
        "\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.data = []\n",
        "    #self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    self.population_size = population_size\n",
        "    self.policy = nn.Sequential(\n",
        "        nn.Linear(4,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,2),\n",
        "        nn.Softmax()\n",
        "        ) # return prob\n",
        "    self.v = nn.Sequential(\n",
        "        nn.Linear(4,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,1),      \n",
        "        )\n",
        "\n",
        "    self.weights = list(self.policy.parameters())\n",
        "    self.sigma = sigma\n",
        "    self.pool = ThreadPool(4)\n",
        "    self.n_rollout = n_rollout\n",
        "    \n",
        "    # v share parameters \n",
        "    # def v(self, x):\n",
        "    #     x = F.relu(self.fc1(x))\n",
        "    #     v = self.fc_v(x)\n",
        "    #     return v\n",
        "    # self.fc1   = nn.Linear(4,256)\n",
        "    # self.fc_pi = nn.Linear(256,2)\n",
        "    # self.fc_v  = nn.Linear(256,1)\n",
        "    \n",
        "  def put_data(self, transition):\n",
        "      self.data.append(transition)\n",
        "      \n",
        "  def make_batch(self):\n",
        "      s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
        "      for transition in self.data:\n",
        "          s, a, r, s_prime, prob_a, done = transition\n",
        "          \n",
        "          s_lst.append(s)\n",
        "          a_lst.append([a])\n",
        "          r_lst.append([r])\n",
        "          s_prime_lst.append(s_prime)\n",
        "          prob_a_lst.append([prob_a])\n",
        "          done_mask = 0 if done else 1\n",
        "          done_lst.append([done_mask])\n",
        "          \n",
        "      s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "                                        torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "                                        torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n",
        "      self.data = []\n",
        "      return s, a, r, s_prime, done_mask, prob_a\n",
        "\n",
        "\n",
        "  def es_step(self):\n",
        "\n",
        "    # population initialization (for differnet epsilon to add )\n",
        "    pops = []\n",
        "    for _ in range(self.population_size):\n",
        "      # N(0,I)\n",
        "      pop = []\n",
        "      for weight in self.weights:\n",
        "        # append parameters from every layer 256 4;256;2 256;2\n",
        "        pop.append(np.random.randn(*weight.data.size()))\n",
        "      pops.append(pop)\n",
        "\n",
        "    #add perturb to form a new distrubution\n",
        "    # new_pops = []\n",
        "    # for pop in pops:\n",
        "    #   new_pop = [np.add(p,self.sigma) for p in pop]\n",
        "    #   new_pops.append(new_pop)\n",
        "    old_weights = copy.deepcopy(self.weights)\n",
        "    new_weights = []\n",
        "    # 两个循环也可以写成 list compresion 加函数\n",
        "    for pop in pops:\n",
        "      for i,weight in enumerate(old_weights):\n",
        "        tmp = torch.from_numpy(self.sigma * pop[i]).float()\n",
        "        new_weights.append(weight.data + tmp)\n",
        "    #print(new_weights)\n",
        "\n",
        "    # to see truth have four nested \n",
        "    # for pop in pops[0]:\n",
        "    #   print(pop)\n",
        "\n",
        "    # evaluate the results from new pops\n",
        "    self.pool.map(self.a2c,new_weights)\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "  def run(self,policy):\n",
        "    '''\n",
        "    run for one time\n",
        "    '''\n",
        "    s = env.reset()\n",
        "    score = 0.0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      for _ in range(self.n_rollout):\n",
        "        prob = policy(torch.from_numpy(s).float())\n",
        "        m = Categorical(prob)\n",
        "        a = m.sample().item()\n",
        "        s_prime, r, done, info = env.step(a)\n",
        "        self.put_data((s,a,r,s_prime,done))\n",
        "        \n",
        "        s = s_prime\n",
        "        score += r\n",
        "        \n",
        "        if done:\n",
        "          break\n",
        "      \n",
        "      # train network\n",
        "      s, a, r, s_prime, done = self.make_batch()\n",
        "      td_target = r + gamma * self.v(s_prime) * done\n",
        "      delta = td_target - self.v(s)\n",
        "      \n",
        "      pi = policy(s)\n",
        "      pi_a = pi.gather(1,a)\n",
        "      # policy loss?\n",
        "      loss = -torch.log(pi_a) * delta.detach() + F.smooth_l1_loss(self.v(s), td_target.detach())\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      self.optimizer.step() \n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "  def a2c(self,weights):\n",
        "\n",
        "    # update by new weight, then use policy to update not weight simulate with a clone policy\n",
        "    simulate_policy = copy.deepcopy(self.policy)\n",
        "    # new weight for simulate policy \n",
        "    for i,simu_weight in enumerate(list(simulate_policy.parameters())):\n",
        "      simu_weight = new_weights[i]\n",
        "\n",
        "    # optimizer?\n",
        "    optimizer = optim.Adam(simulate_policy.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    # simulate with original netwrork\n",
        "    score = self.run(simulate_policy)\n",
        "\n",
        "\n",
        "    #use trained policy to return rewards\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "test = ESPPOModule()"
      ],
      "metadata": {
        "id": "juy_HVH3125m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fir_wei = list(test.policy.parameters())[0]\n",
        "fir_wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tULXN_OBSLVy",
        "outputId": "55609114-5e6f-4302-cbe5-658f750ce7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.1741,  0.0036, -0.0396, -0.4171],\n",
              "        [-0.3754, -0.4730,  0.1517,  0.2199],\n",
              "        [-0.1115, -0.4842, -0.4735,  0.3360],\n",
              "        ...,\n",
              "        [ 0.2666,  0.3873,  0.2126, -0.2084],\n",
              "        [ 0.2393, -0.1507, -0.2080, -0.0130],\n",
              "        [-0.4058, -0.4969,  0.0038, -0.4174]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = [1,2,3,4,5,2,3]\n",
        "lst = []\n",
        "lst.extend(result)\n",
        "lst\n"
      ],
      "metadata": {
        "id": "gvrM4rqaJ6Kd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f328cc8c-b6f9-4c6c-c56e-fc46fe244156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}